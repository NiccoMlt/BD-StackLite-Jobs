%% Direttive TeXworks:
% !TeX root = ../report.tex
% !TEX encoding = UTF-8 Unicode
% !TeX spellcheck = it-IT

% arara: pdflatex: { synctex: yes, shell: yes, interaction: nonstopmode }
% arara: pdflatex: { synctex: yes, shell: yes, interaction: nonstopmode }

\subsection[%
    Job 3: Correlazione tra punteggio e risposte%
  ]{%
    Job 3: Correlazione tra punteggio e risposte tramite Machine Learning%
  }\label{subsec:job3}

  Il terzo e ultimo job concordato richiede l'utilizzo della libreria MLlib di Spark per il calcolo del coefficiente di correlazione
  tra punteggio e numero di risposte utilizzando tecniche di Machine Learning, ignorando le domande cancellate.

  I coefficienti di correlazione supportati da Spark sono il \textit{coefficiente di correlazione lineare di \textbf{Pearson}} e il \textit{coefficiente di correlazione per ranghi di \textbf{Spearman}}.

  Analizzando il dataset, appare chiaro come sia sufficiente caricare solamente il file contenente le domande, in quanto le informazioni relative ai tag non sono necessarie.

  \paragraph{Comando per eseguire il Job}\label{par:job3:cmd}

  \texttt{spark2-submit bd-stacklite-jobs-1.0.0-spark.jar JOBML}

  La classe \texttt{ScalaMain} è stata costruita in modo tale da permettere all'utente di eseguire tutti i Job implementati tramite
  Spark SQL attraverso un unico jar, avendo la possibilità di specificare, tramite parametro, il Job specifico da lanciare.

  Il comando di lancio del Jar accetta inoltre altri tre parametri, che permettono di settare le seguenti configurazioni di Spark\@:
  \begin{itemize}
    \item \texttt{spark.default.parallelism}: di default settato a 8, è il secondo parametro (dopo la specificazione del Job).
    \item \texttt{spark.sql.shuffle.partitions}: di default settato a 8, è il terzo parametro.
    \item \texttt{spark.executor.memory}: di default settato a 5, è il quarto parametro.
  \end{itemize}

  Infine, è possibile utilizzare tutti i parametri standard ammessi dall'operazione submit di Spark\footnote{\url{https://spark.apache.org/docs/2.1.0/submitting-applications.html}},
  in quando la configurazione della SparkSession è solamente estesa.

  \paragraph{Link all'esecuzione su YARN}\label{par:job3:yarn}

  \textbf{TODO}

  \paragraph{File/Tabelle di Input}\label{par:job3:input}

  Le colonne necessarie al raggiungimento dell'obiettivo dell'analisi sono \texttt{Score} ed \texttt{AnswerCount} per il file \texttt{questions.csv};
  durante l'elaborazione, è necessario mantenere anche la colonna \texttt{DeletionDate} per poter ignorare le risposte cancellate

  \paragraph{File/Tabelle di Output}\label{par:job3:output}

  Viene generato un singolo file CSV nella cartella output contenente due linee recanti il nome dell'algoritmo e il relativo valore calcolato separati da virgola.

  \paragraph{Descrizione dell'implementazione}\label{par:job3:implementation}

  La libreria MLlib fornita nel classpath sul server è aggiornata alla versione 2.1.0 e non supporta le Correlation API del package \texttt{org.apache.spark.ml} (il cui supporto è stato introdotto solo dalla versione 2.2.0);
  di conseguenza, si è deciso di utilizzare le Statistics API fornite nel package \texttt{org.apache.spark.mllib}, per quanto consci che sia in via di deprecazione nelle versioni successive.

  \begin{itemize}
    \item
      \textbf{Creazione DataFrame}:
      Come passo iniziale, si è creato il DataFrame caricando il file \texttt{questions.csv} utilizzando la
      classe \texttt{DFBuilder}, che si occupa anche di ricavare lo schema tramite la prima riga e costruire una tabella temporanea.

    \item
      \textbf{Taglio delle colonne non necessarie e rimozione dei valori non validi}:
      Inizialmente, si è proceduto a selezionare le colonne \texttt{DeletionDate}, \texttt{Score} ed \texttt{AnswerCount}, salvando il risultato in un DataFrame.
      Una volta pulito il DataFrame da ogni entry che presentasse un valore differente da \texttt{NA} come data di eliminazione tramite un'operazione di filtro,
      si è proceduto al drop anche della colonna \texttt{DeletionDate}.

      Successivamente, si è provveduto a rimuovere anche le entry con numero di risposte \texttt{NA} o negativo.

    \item
      \textbf{Suddivisione del DataFrame in RDD colonna}:
      Per poter effettuare il calcolo della correlazione, si è proceduto a suddividere il DataFrame in due RDD ciascuno riportante i dati da una delle due colonne;
      per fare ciò, è stato sufficiente selezionare la colonna e mappare la stringa contenuta in ciascuna \texttt{Row} in un valore \texttt{Double}.

    \item
      \textbf{Calcolo dei coefficienti e salvataggio dei risultati}:
      Il calcolo dei coefficienti è immediato tramite metodo \texttt{corr} fornito dalla classe \texttt{Statistics};
      una volta ottenuti entrambi i valori, viene costruito un DataFrame a partire da una sequenza di tuple
      aventi come chiave il nome dell'algoritmo e come valore il rispettivo coefficiente ricavato nei passi precedenti
      e viene scritto su filesystem HDFS in formato CSV.
  \end{itemize}

  \paragraph{Considerazioni sulle performance}\label{par:job3:performance}

  \textbf{TODO}
