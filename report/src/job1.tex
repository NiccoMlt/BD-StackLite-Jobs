%% Direttive TeXworks:
% !TeX root = ../report.tex
% !TEX encoding = UTF-8 Unicode
% !TeX spellcheck = it-IT

% arara: pdflatex: { synctex: yes, shell: yes, interaction: nonstopmode }
% arara: pdflatex: { synctex: yes, shell: yes, interaction: nonstopmode }

\subsection{Job 1: Proporzione giorni feriali/festivi per tag}\label{subsec:job1}
  Il job concordato consiste nel calcolo, per ciascun tag, della proporzione tra la quantità di post che lo utilizzano creati in giorni feriali e giorni festivi, tenendo traccia della quantità totale ai fini dell'ordinamento.

  Durante la fase di studio del dataset, è risultato subito evidente l'esigenza di utilizzare entrambi i file, effettuando un \textit{join} per ID della domanda in modo da poter mettere in relazioni i singoli tag con le informazioni relative ai giorni di apparizione;
  il passo logico successivo è il raggruppamento per tag ai fini del calcolo del numero di apparizioni e della proporzione.

  Si è inoltre ritenuto non necessario mantenere la data di creazione come stringa nella pipeline, in quanto più che sufficiente un booleano che modellasse la festività o meno del giorno di creazione;
  per verificare se un giorno è festivo o meno si è deciso di appoggiarsi alla libreria \textit{Jollyday}\footnote{\url{http://jollyday.sourceforge.net/}} utilizzando il calendario italiano come riferimento per le festività.

  \subsubsection{MapReduce implementation}\label{subsub:job1:mapreduce}

  \paragraph{Comando per eseguire il Job}\label{par:job1:mapreduce:cmd}

  \texttt{hadoop jar bd-stacklite-jobs-1.0.0-mr1.jar}

  Essendo la classe \textit{main} eseguita tramite \texttt{ToolRunner} di Hadoop, supporta il parsing di parametri standard\footnote{\url{https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/CommandsManual.html}} quali ad esempio:
  \begin{itemize}
    \item \texttt{-conf} per caricare una configurazione esterna;
    \item \texttt{-D} per specificare proprietà specifiche per la configurazione;
    \item \texttt{-h} o \texttt{--help} per stampare le opzioni accettate.
  \end{itemize}

  Inoltre, è possibile specificare come primo parametro il path della cartella in ingresso e come secondo paramtero quello della cartella in uscita.

  \paragraph{Link all'esecuzione su YARN}\label{par:job1:mapreduce:yarn}

  \textbf{TODO}

  \paragraph{File/Tabelle di Input}\label{par:job1:mapreduce:input}

  Le colonne necessarie al raggiungimento dell'obiettivo dell'analisi sono \texttt{Id} e \texttt{CreationDate} per il file \texttt{questions.csv}
  e \texttt{Id} e \texttt{Tag} per \texttt{question\_tags.csv}.

  Se non si effettua override dei percorsi tramite parametri di lancio, i file si trovano al percorso specificato nella \Cref{sec:preparation}.

  \paragraph{File/Tabelle di Output}\label{par:job1:mapreduce:output}

  Viene generato un output per ogni passo di MapReduce che viene concluso;
  se non si effettua override dei percorsi tramite parametri di lancio, i file si trovano al percorso specificato nella \Cref{sec:preparation}.

  \textbf{TODO}

  L'output finale è nella forma: \textbf{TODO}

  \paragraph{Descrizione dell'implementazione}\label{par:job1:mapreduce:implementation}

  TODO

  \paragraph{Considerazioni sulle performance}\label{par:job1:mapreduce:performance}

  TODO

  \subsubsection{Spark SQL implementation}\label{subsub:job1:spark}

  \paragraph{Comando per eseguire il Job}\label{par:job1:spark:cmd}

  \texttt{spark2-submit bd-stacklite-jobs-1.0.0-spark.jar JOB1}

  La classe \texttt{ScalaMain} è stata costruita in modo tale da permettere all'utente di eseguire tutti i Job implementati tramite
  Spark SQL attraverso un unico jar, avendo la possibilità di specificare, tramite parametro, il Job specifico da lanciare.
  I differenti Job sono definiti dai seguenti parametri:
  \begin{itemize}
    \item JOB1
    \item JOB2
    \item JOBML
  \end{itemize}

  Il comando di lancio del Jar accetta, inoltre altri tre parametri, che permettono di settare le seguenti configurazioni di Spark SQL:
  \begin{itemize}
    \item \texttt{spark.default.parallelism} : di default settato a 8, è il secondo parametro (dopo la specificazione del Job).
    \item \texttt{spark.sql.shuffle.partitions} : di default settato a 8, è il terzo parametro.
    \item \texttt{spark.executor.memory} : di default settato a 11, è il quarto parametro.
  \item \end{itemize}

\paragraph{Link all'esecuzione su YARN}\label{par:job1:spark:yarn}

  TODO

\paragraph{File/Tabelle di Input}\label{par:job1:spark:input}

  Le colonne necessarie al raggiungimento dell'obiettivo dell'analisi sono \texttt{Id} e \texttt{CreationDate} per il file \texttt{questions.csv}
  e \texttt{Id} e \texttt{Tag} per \texttt{question\_tags.csv}.

  I due file si trovano al percorso specificato nella \Cref{sec:preparation}.

\paragraph{File/Tabelle di Output}\label{par:job1:spark:output}

  Il Job genera un Data Frame di output, che viene salvato come tabella (nominata \texttt{FinalTableJob1})
  sulla piattaforma Hive all'interno del database \texttt{lsemprini_nmaltoni_stacklite_db}.

\paragraph{Descrizione dell'implementazione}\label{par:job1:spark:implementation}

  L'implementazione del Job è stata realizzata utilizzando SparkSQL, contenuta all'interno del metodo \texttt{executeJob()}
  della classe \texttt{it.unibo.bd1819.daysproportion.Job1Main}.
  Il problema è stato affrontato attraverso diversi steps successivi, descritti di seguito:

  \begin{itemize}
    \item \textbf{Creazione DataFrame} : Come passo iniziale, ovviamente siamo partiti dalla creazione dei Data Frame iniziali
    necessari all'ottenimento delle informazioni principali; questa operazione viene svolta interamente da un metodo presente nella
    classe \texttt{DFBuilder}, che permette di leggere i due file di input del dataset (\texttt{question\_tags.csv} e
    \texttt{questions.csv}), ricavarne lo schema tramite la prima riga, generando l'RDD e creare il Data Frame definitivo,
    salvandolo come tabella temporanea.
    \item \textbf{Mapping delle date di creazione} : Il secondo step consiste nell'ottenere un Data Frame che associ, a partire dalla
    tabella \texttt{questions} un Id della domanda ad un booleano che mi dica se la data di creazione (campo \texttt{CreationDate})
    di quella domanda sia feriale o festiva.
    Inizialmente vengono selezionate dalla tabella \texttt{questions} solo le colonne necessarie, ovvero \texttt{Id} e
    \texttt{CreationDate}, poi vengono mappate le righe ottenute: il primo field della riga viene lasciato intatto (in quanto gli
    Id devono essere mantenuti tali), ed il secondo viene dato in pasto ad un metodo della classe \texttt{DateUtils}, che,
    appoggiandosi alla libreria \textit{Jollyday} trasformerà quel campo in un valore booleano (\texttt{true} se la data è
    feriale, \texttt{false} se festiva).
    Il Data Frame risultante da questo primo step viene poi salvato in cache, per ottimizzare le operazioni successive.
    \item \textbf{Join con \texttt{question\_tags}} : Il Data Frame ottenuto dallo step precedente viene subito messo in join
    con il Data Frame corrispondente al file \texttt{question\_tags.csv}, in quanto dovrò associare il valore booleano ad ogni
    campo \texttt{Tag}.
    A questo punto viene tagliato dal Data Frame risultante il campo \texttt{Id}, in quanto inutile per gli steps successivi, per
    rendere più leggera la mole di dati da analizzare.
    Questo Data Frame viene salvato come tabella temporanea dal nome \texttt{dateTagDF}.
    \item \textbf{Creazione del Data Frame finale} : L'obiettivo di questo step è creare un Data Frame che associ ad ogni tag
    il rapporto tra quante domande con quel tag sono state postate in giorni feriali e quante nei giorni festivi.
    Il DF dovrà avere, inoltre, associato ad ogni tag, il conteggio di quante volte quello specifico Tag appare nel dataset.
    Per effettuare l'operazione è stata dunque utilizzata la seguente query:
    \texttt{SELECT tag, (ROUND(
    (CAST(SUM(CASE WHEN IsWorkDay = true THEN 1 ELSE 0 END) AS float)) /
    (CAST(SUM(CASE WHEN IsWorkDay = false THEN 1 ELSE 0 END) AS float)), 2)
    ) AS Proportion,
    COUNT(*) AS Count
    FROM dateAndTagDF GROUP BY tag}

    Il Data Frame ottenuto sarà il risultato finale del Job e viene salvato come tabella su Hive tramite il metodo
    \texttt{saveAsTable()}.
  \end{itemize}

  \paragraph{Considerazioni sulle performance}\label{par:job1:spark:performance}

  Il Job implementato in Spark SQL impiega circa 5 minuti per essere portato a compimento.
  Tempo di esecuzione che è stato pressoché dimezzato da alcuni accorgimenti apportati durante la fase di ottimizzazione.
  Evitare di salvare in cache eccessivamente i Data Frame ottenuti dalle varie operazioni ha aiutato molto a migliorare
  le performance complessive.
  Un altra intuizione è stata quella di accorpare le operazioni descritte nell' ultimo step in un'unica query, quando inizialmente
  veniva eseguito a parte il conteggio dei Tag, cosa che, è stato riscontrato, andava ad inficiare le performance di esecuzione.

  TODO?
