%% Direttive TeXworks:
% !TeX root = ../report.tex
% !TEX encoding = UTF-8 Unicode
% !TeX spellcheck = it-IT

% arara: pdflatex: { synctex: yes, shell: yes, interaction: nonstopmode }
% arara: pdflatex: { synctex: yes, shell: yes, interaction: nonstopmode }

\subsection[%
    Job 2: Suddividere tag in base a score e risposte%
  ]{%
    Job 2: Suddividere i tag in 4 bin per score e risposte e ottenere i top 10 per ciascun bin%
  }\label{subsec:job2}

  Il secondo job concordato consiste nella suddivisione dei tag in 4 sulla base dei valori di score e numero di risposte:

  \begin{itemize}
    \item score basso, numero di risposte basso
    \item score basso, numero di risposte alto
    \item score alto, numero di risposte basso
    \item score alto, numero di risposte alto
  \end{itemize}

  Per ogni bin è richiesto visualizzare la lista dei primi 10 tag per numero di apparizioni.

  Durante la fase di studio del dataset, è risultato subito evidente l'esigenza di utilizzare entrambi i file,
  effettuando un \textit{join} per ID della domanda in modo da poter mettere in relazioni i singoli tag con punteggio e numero di risposte.

  Una volta messo in relazione ciascuna apparizione di ogni tag con il punteggio e il numero di risposte ottenuto (che permettono la scelta del bin di appartenenza),
  è sufficiente raggruppare per tag e per bin per poter effettuare un conteggio.

  Infine, raggruppando unicamente per bin è possibile ottenere una lista di tag con relativo numero di apparizioni.

  \subsubsection{MapReduce implementation}\label{subsub:job2:mapreduce}

  \paragraph{Comando per eseguire il Job}\label{par:job2:mapreduce:cmd}

  \texttt{hadoop jar bd-stacklite-jobs-1.0.0-mr2.jar}

  Essendo la classe \textit{main} eseguita tramite \texttt{ToolRunner} di Hadoop,
  supporta il parsing di parametri standard di Hadoop\footnote{\url{https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/CommandsManual.html}}, quali ad esempio:
  \begin{itemize}
    \item \texttt{-conf} per caricare una configurazione esterna;
    \item \texttt{-D} per specificare proprietà specifiche per la configurazione;
    \item \texttt{-h} o \texttt{--help} per stampare le opzioni accettate.
  \end{itemize}

  Inoltre, è possibile specificare come primo parametro il path della cartella in ingresso e come secondo paramtero quello della cartella in uscita.

  \paragraph{Link all'esecuzione su YARN}\label{par:job2:mapreduce:yarn}

  \textbf{TODO}

  \paragraph{File/Tabelle di Input}\label{par:job2:mapreduce:input}

  Le colonne necessarie per l'analisi sono \texttt{Id}, \texttt{Score} e \texttt{AnswerCount} per il file \texttt{questions.csv}
  e \texttt{Id} e \texttt{Tag} per \texttt{question\_tags.csv}.

  Se non si effettua override dei percorsi tramite parametri di lancio, i file si trovano al percorso specificato nella \Cref{sec:preparation}.

  \paragraph{File/Tabelle di Output}\label{par:job2:mapreduce:output}

  Viene generato un output per ogni passo di MapReduce che viene concluso;
  se non si effettua override dei percorsi tramite parametri di lancio, i file si trovano al percorso specificato nella \Cref{sec:preparation}.

  L'output finale è composto da 4 file nella cartella di output, ciascuno contenente i top 10 tag per numero di apparizioni in quel bin;
  ciascuna riga del file contiene il nome del bin separata con un tab dal nome del tag e il relativo contatore, separati tra loro da virgola.

  \paragraph{Descrizione dell'implementazione}\label{par:job2:mapreduce:implementation}

  Anche questo job è realizzato con tre passi di MapReduce.
  Il primo passo prevede il caricamento di entrambi i file:
  \begin{itemize}
    \item
      nella fase di Map, il file contenente i tag viene caricato direttamente senza manipolazione delle informazioni caricate,
      mentre il secondo file è mappato su una coppia \textit{key-value} avente come chiave l'ID della domanda
      e come valore una coppia contenente il punteggio e il numero di risposte.

    \item
      nella fase di Reduce, viene effettuato il \textit{join} tra le informazioni caricate dai due file sulla base dell'ID della domanda,
      in modo da avere in output coppie \textit{key-value} aventi come chiave il tag e come valore la coppia di cui sopra.
  \end{itemize}

  Il secondo passo ci si occupa di effettuare i calcoli richiesti per l'analisi:
  \begin{itemize}
    \item
      nella fase di Map, l'output del passo precedente viene caricato mappando la coppia costituita da punteggio e numero di risposte in uno dei 4 bin.
    \item
      la fase di Reduce riceve dunque in ingresso le coppie chiave-valore che mettono in relazione i singoli tag con il bin di appartenenza, aggregate per tag;
      in questa fase è stato dunque possibile mantenere, per ogni tag, una mappa che conta il numero di apparizioni di ciascun tag in ciascun bin.

      In output a questa fase sono prodotte coppie \textit{key-value} aventi come chiave la coppia di tag e bin e come valore il contatore.
  \end{itemize}

  Infine, nel terzo passo, i dati vengono raccolti e ordinati:
  \begin{itemize}
    \item
      nella fase di Map, le coppie chiave-valore in output alla fase precedente vengono manipolate
      al fine di generare, per ciascuna, nuove coppie aventi come chiave il bin e come valore la coppia di tag e numero di apparizioni.
    \item
      nella fase di Reduce, per ciascun bin le coppie in uscita dalla fase di Map vengono raccolte in una mappa;
      in questo modo, al termine della riduzione, la lista di \textit{entry} della mappa viene ordinata e vengono emesse in output le prime 10 entry per ogni bin.
  \end{itemize}

  \paragraph{Considerazioni sulle performance}\label{par:job2:mapreduce:performance}

  \textbf{TODO}

  \subsubsection{Spark SQL implementation}\label{subsub:job2:spark}

  \paragraph{Comando per eseguire il Job}\label{par:job2:spark:cmd}

  \textbf{TODO}

  \paragraph{Link all'esecuzione su YARN}\label{par:job2:spark:yarn}

  \textbf{TODO}

  \paragraph{File/Tabelle di Input}\label{par:job2:spark:input}

  \textbf{TODO}

  \paragraph{File/Tabelle di Output}\label{par:job2:spark:output}

  \textbf{TODO}

  \paragraph{Descrizione dell'implementazione}\label{par:job2:spark:implementation}

  \textbf{TODO}

  \paragraph{Considerazioni sulle performance}\label{par:job2:spark:performance}

  \textbf{TODO}
